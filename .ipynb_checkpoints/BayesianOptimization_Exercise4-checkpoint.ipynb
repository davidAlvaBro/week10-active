{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Q1wnryKmxkOz",
    "outputId": "f719230f-6b4c-4861-e404-884ecc7d57ec"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#Install packages prefer conda if you are using anaconda otherwise try with pip, uncomment only if install is needed (if you use Google colab pip should work)\n",
    "\n",
    "#conda:\n",
    "#os.system('conda install GPy -c conda-forge')\n",
    "#os.system('conda install GPyOpt -c conda-forge')\n",
    "\n",
    "#pip:\n",
    "#!{sys.executable} -m pip install GPy\n",
    "#!{sys.executable} -m pip install GPyOpt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GPyOpt\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterSampler, RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "import random\n",
    "\n",
    "np.random.seed(32)\n",
    "random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6ZbPxoDxkPB"
   },
   "source": [
    "# Hyperparameters tuning using Bayesian Optimization with the GPyOpt library\n",
    "\n",
    "As you are going to see, the project for this first part of the course will be to use Bayesian Optimization to tune the hyperparameters of a Deep Neural Network. To do that you are going to use a library that is more optimized compare to the code from scratch that we have written before. We are going to use [GPyOPt](https://sheffieldml.github.io/GPyOpt/) developed by the Machine Learning group of the University of Sheffield and it is based on GPy, which is a framework for using Gaussian Process in Python.\n",
    "\n",
    "In this exercise you are going to use Bayesian Optimization to select the best hyperparameters of a random forest trained on part of the MNIST dataset. You are going also to compare it with respect to a random search on the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uup6huN4xkPJ"
   },
   "outputs": [],
   "source": [
    "def load_MNIST():\n",
    "    '''\n",
    "    Function to load the MNIST training and test set with corresponding labels.\n",
    "\n",
    "    :return: training_examples, training_labels, test_examples, test_labels\n",
    "    '''\n",
    "\n",
    "    # we want to flat the examples\n",
    "\n",
    "    training_set = datasets.MNIST(root='./data', train=True, download=True, transform= None)\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform= None)\n",
    "\n",
    "    Xtrain = training_set.data.numpy().reshape(-1,28*28)\n",
    "    Xtest = test_set.data.numpy().reshape(-1,28*28)\n",
    "\n",
    "    ytrain = training_set.targets.numpy()\n",
    "    ytest = test_set.targets.numpy()\n",
    "\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIx5w0wjxkPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Using downloaded and verified file: ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Using downloaded and verified file: ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Using downloaded and verified file: ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "Information about the new datasets\n",
      "Training set shape: (60000, 784)\n",
      "Test set shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "## since training a random forest on the entire dataset takes some time\n",
    "## we can consider only few labels, like 3, 5, 8 and 9\n",
    "\n",
    "## we can load the training set and test set\n",
    "Xtrain, ytrain, Xtest, ytest = load_MNIST()\n",
    "\n",
    "## we use a mask to selects those subsets\n",
    "#train_filter = np.isin(ytrain, [3, 5, 8, 9])\n",
    "#test_filter = np.isin(ytest, [3, 5, 8, 9])\n",
    "\n",
    "# apply the mask to the entire dataset\n",
    "#Xtrain, ytrain = Xtrain[train_filter], ytrain[train_filter]\n",
    "#Xtest, ytest = Xtest[test_filter], ytest[test_filter]\n",
    "\n",
    "# print some information\n",
    "print('Information about the new datasets')\n",
    "print('Training set shape:', Xtrain.shape)\n",
    "print('Test set shape', Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLelVc36xkPa"
   },
   "source": [
    "## Random forest\n",
    "\n",
    "As you have seen in your Machine Learning course, there are a lot of hyperparameters to choose before training a Random Forest. If you look at the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) you can notice all the paramters. For this exercise we will focus only on the following four hyperparameters:\n",
    "\n",
    "1. **n_estimators**: the number of decision trees that are in the forest;\n",
    "2. **criterion**: the criterion to evaluate the split. We should decide between *Gini impurity* (`gini`) and *information gain* (`entropy`);\n",
    "3. **max_depth**: Maximum depth of the trees. If None, the tree expands until we have a element in all the leaves or  until all leaves contain less than min_samples_split samples.\n",
    "4. **max_features**: The number of features to consider when looking for the best split. We have to choose among `'sqrt'` where we consider `sqrt(n_features)`, `'log2'` where we consider `log2(n_features)`. It is possible to use `None` where we consider all the features, but it makes everything super slow, so we are avoiding it.\n",
    "\n",
    "Remember that when we are using Random Forest we can avoid running the cross-validation to get the validation error as approximation of the test error, but instead we can use the *out of bag* error to get an approximation of the test error we are going to get when we consider unseen example. \n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "1. You should define a `RandomForestClassifier` with default parameters, and compute the *out_of_bag* error (you should use `oob_score=True` when you define the classifier) and then the test error. This would be the baseline for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKz2Nq5OxkPc"
   },
   "outputs": [],
   "source": [
    "model = \n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Default model out of bag error', model.oob_score_)\n",
    "print('Default model test accuracy', model.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2DIwTl5xkPk"
   },
   "source": [
    "### Random search \n",
    "\n",
    "A possible way to find the best hyperparameters is to use random search over the parameter space. To perform this operation we can use `RandomizedSearchCV`. You can look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). However, since we do not need cross-validation for Random Forests, we can use `ParameterSampler` to sample random parameters from our parameter space. Look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler) To be able to use this method you should first define a dictionary of the hyperparameters you want to optimize, in this case the four we mentioned above, and then decide a way to evaluate the model and how many folds for the cross-validation. The dictionary is of the form:\n",
    "\n",
    "```python\n",
    "params = {\"name_params\": uniform(0, 1), # if it is continuous between 0 and 1\n",
    "           \"name_params2\": range(1,50), # if it is discrete but integer\n",
    "           \"name_params3\": ['name1', 'name2']} # if it is discrete but string`\n",
    "```\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'> Crete the dictionary of the hyperparameters, we recommend keeping the value of the number of trees less than 100 otherwise it takes a very long time. You can try to increase it at home, but it will likely not be possible in the exercise session as you would be stuck for some time. After that, you can use the dictionary to create a random hyperparameter using `ParameterSampler`. The `ParameterSampler` call is already provided.\n",
    "\n",
    "2. <font color='blue'> Using the parameters returned by the `ParameterSampler` you should fit a random forest, and for each iteration you should store the best value of the `model.oob_score_` you get. **HINT:** You can access the hyperparameters value by the name you used in the defined dictionary. (The whole process would take like 5/10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQP9z8AOxkPm"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# hyperparams dictionary \n",
    "domain = \n",
    "#rs = RandomizedSearchCV(model, param_distributions=domain, cv=3, verbose =2, n_iter=10)\n",
    "#rs.fit(Xtrain, ytrain)\n",
    "\n",
    "# create the ParameterSampler\n",
    "param_list = list(ParameterSampler(domain, n_iter=20, random_state=32))\n",
    "print('Param list')\n",
    "print(param_list)\n",
    "#rounded_list = [dict((k,v) for (k, v) in d.items()) for d in param_list]\n",
    "\n",
    "#print('Random parameters we are going to consider')\n",
    "#print(rounded_list)\n",
    "\n",
    "## now we can train the random forest using these parameters tuple, and for\n",
    "## each iteration we store the best value of the oob\n",
    "\n",
    "current_best_oob = 0\n",
    "iteration_best_oob = 0 \n",
    "max_oob_per_iteration = []\n",
    "i = 0\n",
    "for params in param_list:\n",
    "    print(i)\n",
    "    print(params)\n",
    "    model = \n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    end = time.time()\n",
    "    model_oob = model.oob_score_\n",
    "    print('OOB found:', model_oob)\n",
    "    if model_oob > current_best_oob:\n",
    "        current_best_oob = model_oob\n",
    "        iteration_best_oob = i\n",
    "    \n",
    "    max_oob_per_iteration.append(current_best_oob)\n",
    "    i += 1\n",
    "    print(f'It took {end - start} seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NmwnVJ2xkPt"
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "The procedure we are interested in, instead, is Bayesian Optimization. As before, also for GPyOpt, we should start by defining the set of hyperparameters you want to optimize. If some hyperparameters are discrete and represented by a string, you should integer instead and change them when you are initializing the classifier. The dictionary should have this form `{'name': 'name_of_params', 'type': 'type_of_variable', 'domain': domain_given_as_tuple}`. Note that the domain should specify as tuple. An example is given by\n",
    "\n",
    "```python\n",
    "params = [{'name': 'var_1', 'type': 'continuous', 'domain': (0,10)},\n",
    "            {'name': 'var_2', 'type': 'discrete', 'domain': (0,5,10)},\n",
    "            {'name': 'var_3', 'type': 'categorical', 'domain': (0,1)}]`\n",
    "```\n",
    "\n",
    "In the example above, suppose that `'var_3'` is usually used as string, where `0 = 'l1'` and `1 = 'l2'`, for example.\n",
    "If you want to define a large discrete interval, you should define the tuple as `tuple(np.arange(1,100,1, dtype= np.int))`. \n",
    "\n",
    "The second ingredient we should define is the objective function we want to maximize. Since GPyOpt minimize things, when you are returning the value of the objective you return it negated. This function, usually defined as `objective_function(x)`, takes a general parameter `x`. This represents all the parameter we need. Therefore, when we define the function, we should collect the parameters by doing `param = x[0]`. Then we are able to pass the parameters to the classifier. In the example above we will have that `param[0]` is the 'var_1', `param[0]` is 'var_2', whereas, since 'var_3' is usually a string we should get back the string value, therefore `if param[2] == 0: var_3 = l1 else: var_3 = 'l2'`.\n",
    "Note that by default GPyOpt initializes with 5 points and considers Bayesian optimization after this, therefore max_iter=15 will give you a total of 20 function evaluation. \n",
    "In a general way, the scheme for the objective function is similar to:\n",
    "\n",
    "```python\n",
    "def objective_function(x):\n",
    "\n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 1:\n",
    "        val_3 = 'l1'\n",
    "    else:\n",
    "        val_3 = 'l2'\n",
    "        \n",
    "    model = your_model(param[0],param[1],val_3)\n",
    "    model.fit(X,y)\n",
    "    \n",
    "    ## then you will compute the measure you want to maximize\n",
    "    ## usually you should consider a validation set\n",
    "    ## in case of random forest you return the - oob score\n",
    "    return - model.score(Xvalid, yvalid)\n",
    "```\n",
    "\n",
    "In case of random forest, you should return the `oob_score` and not the validation score. If you are going to use other models you should consider the validation set, instead.\n",
    "\n",
    "When you have these two ingredients, you should create an Bayesian Optimization instancer using the following function `GPyOpt.methods.BayesianOptimization`, see the complete description [here](https://gpyopt.readthedocs.io/en/latest/GPyOpt.methods.html). Run it calling the `run_optimization` function of this instance, documentation [here](https://gpyopt.readthedocs.io/en/latest/GPyOpt.core.html).\n",
    "\n",
    "At the end of the optimization, you can access the array of the best parameters by `x_best = opt.X[np.argmin(opt.Y)]`. You can also collect the best `oob_score` per iteration using \n",
    "`y_bo = np.maximum.accumulate(-opt.Y).ravel()`.\n",
    "\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'>You should define all the ingredients needed for GPyOpt: 1) define the dictionary of hyperparameters, following the GPyOpt rules; 2) define the objective function you want to optimize. GPyOpt usually minimize (there is a parameter called maximize but it appears to have no effect), therefore, since in your setting you want to maximize the oob_score, you should return the negative oob_score; 3) run the optimization. When you create the `GPyOpt.methods.BayesianOptimization` instance you should define the following parameters: `f`,`domain`, `acquisition_type`, `exploration_weight`. The documentation for the GPyOpt package is quite limited and it is quite unclear to define how to define certain parameter, so elaborate hints are given here. The fitting procedure should take $\\sim 10$ minutes. Do not worry about the warning \"The set cost function is ignored! LCB acquisition does not make sense with cost.\" which might occur when using LCB. From discussions on github it appears to be a known error and might be fixed in the lastest git master branch.\n",
    "\n",
    "2. <font color='blue'>In the same plot, show the best `oob_score` per iteration you obtain using the random search and the bayesian optimization. What do you see?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2TTDArnxkPv"
   },
   "outputs": [],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,101,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(10,110,10, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = \n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "\n",
    "    #fit the model\n",
    "    model = \n",
    "    print(model.oob_score_)\n",
    "    return - model.oob_score_\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = ,      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 15) \n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DvhNlzcxkP2"
   },
   "outputs": [],
   "source": [
    "## comparison between random search and bayesian optimization\n",
    "## we can plot the maximum oob per iteration of the sequence\n",
    "\n",
    "# collect the maximum each iteration of BO, note that it is also provided by GPOpt in Y_Best\n",
    "y_bo = \n",
    "# define iteration number\n",
    "xs = \n",
    "\n",
    "plt.plot(xs, max_oob_per_iteration, 'o-', color = 'red', label='Random Search')\n",
    "plt.plot(xs, y_bo, 'o-', color = 'blue', label='Bayesian Optimization')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Out of bag error')\n",
    "plt.title('Comparison between Random Search and Bayesian Optimization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6dHDP0rU-5r"
   },
   "source": [
    "# [OPTIONAL] Investigate the acquisition function (not part of the exam)\n",
    "<br>\n",
    "<font color='blue'>\n",
    "1. <font color='blue'> Investigate the dimensionality of the GP (look at the model subclass), how many dimensions are there? Why?\n",
    " \n",
    "2.<font color='blue'> Take a look at the acqusition function, as this is more than 2 dimensional in this case it is not trivial to plot it, therefore keep the categorical variables fixed at some value that you decide and evaluate the acquisition function on a grid for the remaining two variables. The acquisition function can be evaluated using opt.acquisition.acquisition_function and you can create the grid using np.meshgrid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ666LjBQZPG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture_2_Exercise2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
